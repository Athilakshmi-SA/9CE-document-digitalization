{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0816bdcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7a2a457",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [WinError 10054] An\n",
      "[nltk_data]     existing connection was forcibly closed by the remote\n",
      "[nltk_data]     host>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10054]\n",
      "[nltk_data]     An existing connection was forcibly closed by the\n",
      "[nltk_data]     remote host>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Name': 'dinesh', 'email': ['pgma25l@gmail.com'], 'contact': ['8838336854']}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import nltk\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define PDF file path\n",
    "pdf_file = r\"D:\\\\Dinesh (2).pdf\"\n",
    "\n",
    "# Convert PDF to text using OCR\n",
    "images = convert_from_path(pdf_file)\n",
    "output_dir = 'temp_images'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "text_list = []\n",
    "\n",
    "for i, image in enumerate(images):\n",
    "    image_path = os.path.join(output_dir, f\"page_{i + 1}.png\")\n",
    "    image.save(image_path, 'PNG')\n",
    "    text = pytesseract.image_to_string(Image.open(image_path))\n",
    "    text_list.append(text)\n",
    "\n",
    "# Clean up temporary image files\n",
    "for image_path in os.listdir(output_dir):\n",
    "    os.remove(os.path.join(output_dir, image_path))\n",
    "os.rmdir(output_dir)\n",
    "\n",
    "# Combine extracted text from images\n",
    "txt = \"\\n\".join(text_list)\n",
    "\n",
    "# Function to extract names\n",
    "def extract_names(txt):\n",
    "    person_names = []\n",
    "\n",
    "    for sent in nltk.sent_tokenize(txt):\n",
    "        words = nltk.word_tokenize(sent)\n",
    "        tagged_words = nltk.pos_tag(words)\n",
    "        chunked = nltk.ne_chunk(tagged_words)\n",
    "        \n",
    "        for subtree in chunked:\n",
    "            if isinstance(subtree, nltk.Tree) and subtree.label() == 'PERSON':\n",
    "                person_names.append(' '.join([leaf[0] for leaf in subtree.leaves()]))\n",
    "\n",
    "    return person_names\n",
    "\n",
    "names = extract_names(txt)\n",
    "name_candidate = names[0] if names else \"\"\n",
    "\n",
    "# Function to extract phone numbers\n",
    "def extract_phone_number(resume_text):\n",
    "    PHONE_REG_IND = re.compile(r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]')\n",
    "    phone = re.findall(PHONE_REG_IND, resume_text)\n",
    "\n",
    "    if phone:\n",
    "        number = ''.join(phone[0])\n",
    "\n",
    "        if resume_text.find(number) >= 0 and len(number) < 16:\n",
    "            return number\n",
    "    return None\n",
    "\n",
    "phone_number_ind = extract_phone_number(txt)\n",
    "\n",
    "def extract_phone_number_usa(resume_text):\n",
    "    PHONE_REG_USA = re.compile(r'^\\(?(\\d{3})\\)?[-]?(\\d{3})[-]?(\\d{4})$')\n",
    "    phone = re.findall(PHONE_REG_USA, resume_text)\n",
    "\n",
    "    if phone:\n",
    "        if resume_text.find(phone[0]) >= 0:\n",
    "            return phone[0]\n",
    "    return None\n",
    "\n",
    "phone_number_usa = extract_phone_number_usa(txt)\n",
    "\n",
    "phone_contact = []\n",
    "if phone_number_ind:\n",
    "    phone_contact.append(phone_number_ind)\n",
    "if phone_number_usa:\n",
    "    phone_contact.append(phone_number_usa)\n",
    "\n",
    "# Function to extract emails\n",
    "EMAIL_REG = re.compile(r'[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+')\n",
    "def extract_emails(resume_text):\n",
    "    return re.findall(EMAIL_REG, resume_text)\n",
    "\n",
    "emails = extract_emails(txt)\n",
    "\n",
    "# Dictionary to store general information\n",
    "\n",
    "general_dict = {\n",
    "    'Name': name_candidate.lower(),\n",
    "    'email': emails,\n",
    "    'contact': phone_contact\n",
    "}\n",
    "\n",
    "# Print general_dict\n",
    "print(general_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a815929",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ajayk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ajayk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc     Name                   email    contact domain skills  normalized_count  total_skills\n",
      "    MITHULAN mithulanr2003@gmail.com 7845402642     []      R             33.33             3\n",
      "    MITHULAN mithulanr2003@gmail.com 7845402642     []   HTML             33.33             3\n",
      "    MITHULAN mithulanr2003@gmail.com 7845402642     []    CSS             33.33             3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define PDF file path\n",
    "pdf_file = r\"C:\\Users\\ajayk\\OneDrive\\Documents\\project\\mithulan_resume.pdf\"\n",
    "\n",
    "# Convert PDF to text using OCR\n",
    "images = convert_from_path(pdf_file)\n",
    "output_dir = 'temp_images'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "text_list = []\n",
    "\n",
    "for i, image in enumerate(images):\n",
    "    image_path = os.path.join(output_dir, f\"page_{i + 1}.png\")\n",
    "    image.save(image_path, 'PNG')\n",
    "    text = pytesseract.image_to_string(Image.open(image_path))\n",
    "    text_list.append(text)\n",
    "\n",
    "# Clean up temporary image files\n",
    "for image_path in os.listdir(output_dir):\n",
    "    os.remove(os.path.join(output_dir, image_path))\n",
    "os.rmdir(output_dir)\n",
    "\n",
    "# Combine extracted text from images\n",
    "txt = \"\\n\".join(text_list)\n",
    "\n",
    "# Function to extract names\n",
    "def extract_names(txt):\n",
    "    person_names = []\n",
    "\n",
    "    for sent in nltk.sent_tokenize(txt):\n",
    "        words = nltk.word_tokenize(sent)\n",
    "        tagged_words = nltk.pos_tag(words)\n",
    "        chunked = nltk.ne_chunk(tagged_words)\n",
    "        \n",
    "        for subtree in chunked:\n",
    "            if isinstance(subtree, nltk.Tree) and subtree.label() == 'PERSON':\n",
    "                person_names.append(' '.join([leaf[0] for leaf in subtree.leaves()]))\n",
    "\n",
    "    return person_names\n",
    "\n",
    "names = extract_names(txt)\n",
    "name_candidate = names[0] if names else \"\"\n",
    "\n",
    "# Function to extract phone numbers\n",
    "def extract_phone_number(resume_text):\n",
    "    PHONE_REG_IND = re.compile(r'[\\+\\(]?[1-9][0-9 .\\-\\(\\)]{8,}[0-9]')\n",
    "    phone = re.findall(PHONE_REG_IND, resume_text)\n",
    "\n",
    "    if phone:\n",
    "        number = ''.join(phone[0])\n",
    "\n",
    "        if resume_text.find(number) >= 0 and len(number) < 16:\n",
    "            return number\n",
    "    return None\n",
    "\n",
    "phone_number_ind = extract_phone_number(txt)\n",
    "\n",
    "def extract_phone_number_usa(resume_text):\n",
    "    PHONE_REG_USA = re.compile(r'^\\(?(\\d{3})\\)?[-]?(\\d{3})[-]?(\\d{4})$')\n",
    "    phone = re.findall(PHONE_REG_USA, resume_text)\n",
    "\n",
    "    if phone:\n",
    "        if resume_text.find(phone[0]) >= 0:\n",
    "            return phone[0]\n",
    "    return None\n",
    "\n",
    "phone_number_usa = extract_phone_number_usa(txt)\n",
    "\n",
    "phone_contact = []\n",
    "if phone_number_ind:\n",
    "    phone_contact.append(phone_number_ind)\n",
    "if phone_number_usa:\n",
    "    phone_contact.append(phone_number_usa)\n",
    "\n",
    "# Function to extract emails\n",
    "EMAIL_REG = re.compile(r'[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+')\n",
    "def extract_emails(resume_text):\n",
    "    return re.findall(EMAIL_REG, resume_text)\n",
    "\n",
    "emails = extract_emails(txt)\n",
    "\n",
    "# Dictionary to store general information\n",
    "general_dict = {\n",
    "    'Name': name_candidate.lower(),\n",
    "    'email': emails,\n",
    "    'contact': phone_contact\n",
    "}\n",
    "\n",
    "# Load skills data from an Excel file\n",
    "file_skills_domain = pd.read_excel(r\"C:\\Users\\ajayk\\OneDrive\\Documents\\project\\ResumeSkill.xlsx\", engine=\"openpyxl\")\n",
    "file_skills_domain.columns = file_skills_domain.columns.str.strip().str.upper()\n",
    "\n",
    "list_domains = [col for col in file_skills_domain.columns if col != 'EDUCATION']\n",
    "list_skills = []\n",
    "\n",
    "for col in list_domains:\n",
    "    file_skills_domain[col] = file_skills_domain[col].str.strip().str.upper()\n",
    "    list_skills.extend([x for x in file_skills_domain[col].dropna()])\n",
    "\n",
    "# Initialize skills_dict and domain_list\n",
    "skills_dict = {}\n",
    "domain_list = []\n",
    "\n",
    "# Function to extract skills\n",
    "def extract_skills(input_text):\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    word_tokens = nltk.word_tokenize(input_text)\n",
    "    filtered_tokens = [w for w in word_tokens if w not in stop_words and w.isalpha()]\n",
    "\n",
    "    found_skills = set()\n",
    "    for token in filtered_tokens:\n",
    "        if token.upper() in list_skills:\n",
    "            found_skills.add(token.upper())\n",
    "\n",
    "    for skill in found_skills:\n",
    "        if skill not in skills_dict:\n",
    "            skills_dict[skill] = 1\n",
    "        else:\n",
    "            skills_dict[skill] += 1\n",
    "            for j in list_domains:\n",
    "                if skill in file_skills_domain[j].to_list():\n",
    "                    domain_list.append(j)\n",
    "\n",
    "extract_skills(txt)\n",
    "\n",
    "# Count domain occurrences\n",
    "domain_counts = Counter(domain_list)\n",
    "\n",
    "# Sort domain_counts by count in descending order\n",
    "sorted_domain_counts = domain_counts.most_common()\n",
    "\n",
    "# Check if there are at least two elements in sorted_domain_counts\n",
    "if len(sorted_domain_counts) >= 2:\n",
    "    general_dict[\"domain\"] = [sorted_domain_counts[0][0][5:], sorted_domain_counts[1][0][5:]]\n",
    "else:\n",
    "    general_dict[\"domain\"] = []\n",
    "\n",
    "# Create a list of details for the dataframe\n",
    "list_details = []\n",
    "\n",
    "skill_df = pd.DataFrame.from_dict(skills_dict, orient='index', columns=['Count'])\n",
    "skill_df = skill_df.sort_values(by='Count', ascending=False)\n",
    "num = skill_df['Count'].sum()\n",
    "\n",
    "for skill_x, count in skill_df.iterrows():\n",
    "    normalized_count = round((count['Count'] * 100) / num, 2)\n",
    "    list_details.append({\n",
    "        'doc': '',\n",
    "        'Name': general_dict['Name'].upper(),\n",
    "        'email': general_dict['email'][0] if general_dict['email'] else '',\n",
    "        'contact': general_dict['contact'][0] if general_dict['contact'] else '',\n",
    "        'domain': general_dict['domain'],\n",
    "        'skills': skill_x,\n",
    "        'normalized_count': normalized_count,\n",
    "        'total_skills': len(skill_df)\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(list_details)\n",
    "\n",
    "# Print the DataFrame in tabular format\n",
    "print(df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eba68b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
